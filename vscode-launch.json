{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "QAT W4A8 RTE", 
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                // "CUDA_LAUNCH_BLOCKING": "1",
                // "PYTHONPATH": "${workspaceFolder}/transformer-quantization",
            },
            "cwd": "${workspaceFolder}/transformer-quantization",
            "program": "main.py",
            "args": [
                "train-quantized",
                "--cuda", 
                "--do-eval", 
                "--logging-first-step", 
                "--weight-quant", 
                "--act-quant", 
                "--pad-to-max-length", 
                "--learn-ranges", 
                "--tqdm", 
                "--batch-size", "8", 
                "--seed", "1000", 
                "--model-name", "bert_base_uncased", 
                "--learning-rate", "5e-05", 
                "--num-epochs", "6", 
                "--warmup-steps", "186", 
                "--weight-decay", "0.0", 
                "--attn-dropout", "0.0", 
                "--hidden-dropout", "0.0", 
                "--max-seq-length", "128", 
                "--n-bits", "4", 
                "--n-bits-act", "8", 
                "--qmethod", "symmetric_uniform", 
                "--qmethod-act", "asymmetric_uniform", 
                "--weight-quant-method", "MSE", 
                "--weight-opt-method", "golden_section", 
                "--act-quant-method", "current_minmax", 
                "--est-ranges-batch-size", "16", 
                "--num-est-batches", "1", 
                "--quant-setup", "all", 
                "--model-path", "/tmp/vscode-runs/qcom-peg/saved_models/rte/out", 
                "--task", "rte", 
                "--output-dir", "/tmp/vscode-runs/qcom-peg/qat-rte"
                // To run mixed-precision QAT with 2-bit embeddings and 4-bit weights, add --quant-dict "{'Et': 2}" see README.md for complete documentation
            ]
        },
        {
            "name": "AdaRound PTQ W4A32 RTE", 
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                // "CUDA_LAUNCH_BLOCKING": "1",
                // "PYTHONPATH": "${workspaceFolder}/transformer-quantization",
            },
            "cwd": "${workspaceFolder}/transformer-quantization",
            "program": "main.py",
            "args": [
                "validate-quantized",
                "--weight-quant", 
                "--no-act-quant", 
                "--no-pad-to-max-length", 
                "--est-ranges-no-pad", 
                "--eval-batch-size", "16", 
                "--seed", "1000", 
                "--model-path", "/tmp/vscode-runs/qcom-peg/saved_models/", 
                "--task", "rte", 
                "--qmethod", "symmetric_uniform", 
                "--qmethod-act", "asymmetric_uniform", 
                "--n-bits", "4", 
                "--weight-quant-method", "MSE", 
                "--weight-opt-method", "grid", 
                "--num-candidates", "100", 
                "--quant-setup", "all", 
                "--adaround", "all", 
                "--adaround-num-samples", "1024", 
                "--adaround-init", "range_estimator", 
                "--adaround-mode", "learned_hard_sigmoid", 
                "--adaround-asym", 
                "--adaround-iters", "10000", 
                "--adaround-act-quant", "no_act_quant"
            ]
        },
        {
            "name": "PEG-AQ PTQ RTE", 
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                // "CUDA_LAUNCH_BLOCKING": "1",
                // "PYTHONPATH": "${workspaceFolder}/transformer-quantization",
            },
            "cwd": "${workspaceFolder}/transformer-quantization",
            "program": "main.py",
            "args": [
                "validate-quantized", 
                "--act-quant", 
                "--weight-quant", 
                "--no-pad-to-max-length", 
                "--est-ranges-no-pad", 
                "--eval-batch-size", "16", 
                "--seed", "1000", 
                "--model-path", "/tmp/vscode-runs/qcom-peg/saved_models/", 
                "--task", "rte", 
                "--n-bits", "8", 
                "--n-bits-act", "8", 
                "--qmethod", "symmetric_uniform", 
                "--qmethod-act", "asymmetric_uniform", 
                "--weight-quant-method", "MSE", 
                "--weight-opt-method", "golden_section", 
                "--act-quant-method", "current_minmax", 
                "--est-ranges-batch-size", "1", 
                "--num-est-batches", "1", 
                "--quant-setup", "all",
                "--quant-dict", "{\"y\": \"ngp6\", \"h\": \"ngp6\", \"x\": \"ngp6\"}", 
                "--per-groups-permute-shared-h"
            ]
        },
        {
            "name": "PTQ W8A{8,16} RTE", 
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                // "CUDA_LAUNCH_BLOCKING": "1",
                // "PYTHONPATH": "${workspaceFolder}/transformer-quantization",
            },
            "cwd": "${workspaceFolder}/transformer-quantization",
            "program": "main.py",
            "args": [
                "validate-quantized", 
                "--act-quant", 
                "--weight-quant", 
                "--no-pad-to-max-length", 
                "--est-ranges-no-pad", 
                "--eval-batch-size", "16", 
                "--seed", "1000", 
                "--model-path", "/tmp/vscode-runs/qcom-peg/saved_models/", 
                "--task", "rte", 
                "--n-bits", "8", 
                "--n-bits-act", "8", 
                "--qmethod", "symmetric_uniform", 
                "--qmethod-act", "asymmetric_uniform", 
                "--weight-quant-method", "MSE", 
                "--weight-opt-method", "golden_section", 
                "--act-quant-method", "current_minmax", 
                "--est-ranges-batch-size", "1", 
                "--num-est-batches", "1", 
                "--quant-setup", "all",
                "--quant-dict", "{\"y\": 16, \"h\": 16, \"x\": 16}"
            ]
        },
        {
            "name": "PTQ W8A8 RTE", 
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                // "CUDA_LAUNCH_BLOCKING": "1",
                // "PYTHONPATH": "${workspaceFolder}/transformer-quantization",
            },
            "cwd": "${workspaceFolder}/transformer-quantization",
            "program": "main.py",
            "args": [
                "validate-quantized", 
                "--act-quant", 
                "--weight-quant", 
                "--no-pad-to-max-length", 
                "--est-ranges-no-pad", 
                "--eval-batch-size", "16", 
                "--seed", "1000", 
                "--model-path", "/tmp/vscode-runs/qcom-peg/saved_models/", 
                "--task", "rte", 
                "--n-bits", "8", 
                "--n-bits-act", "8", 
                "--qmethod", "symmetric_uniform", 
                "--qmethod-act", "asymmetric_uniform", 
                "--weight-quant-method", "MSE", 
                "--weight-opt-method", "golden_section", 
                "--act-quant-method", "current_minmax", 
                "--est-ranges-batch-size", "1", 
                "--num-est-batches", "1", 
                "--quant-setup", "all"
            ]
        },
        {
            "name": "Validate RTE", 
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                // "CUDA_LAUNCH_BLOCKING": "1",
                // "PYTHONPATH": "${workspaceFolder}/transformer-quantization",
            },
            "cwd": "${workspaceFolder}/transformer-quantization",
            "program": "main.py",
            "args": [
                "validate-baseline", 
                "--eval-batch-size", "32", 
                "--seed", "1000", 
                "--model-name", "bert_base_uncased", 
                "--model-path", "/tmp/vscode-runs/qcom-peg/saved_models/", 
                "--task", "rte"
            ]
        },
        {
            "name": "Fine Tuning RTE", 
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                // "CUDA_LAUNCH_BLOCKING": "1",
                // "PYTHONPATH": "${workspaceFolder}/transformer-quantization",
            },
            "cwd": "${workspaceFolder}/transformer-quantization",
            "program": "main.py",
            "args": [
                "train-baseline", 
                "--cuda", 
                "--save-model", 
                "--model-name", "bert_base_uncased", 
                "--task", "rte", 
                "--learning-rate", "2e-05", 
                "--batch-size", "8", 
                "--eval-batch-size", "8", 
                "--num-epochs", "3", 
                "--max-seq-length", "128", 
                "--seed", "1000", 
                "--output-dir", "/tmp/vscode-runs/qcom-peg/saved_models/rte"
            ]
        },
    ]
}